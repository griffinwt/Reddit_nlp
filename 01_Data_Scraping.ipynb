{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Data Scraping  \n",
    "(Problem statement here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to write a function that pulls at least 10,000 posts from each subreddit and then saves that in a dataframe which I can save as a csv. I'll need to use one or more \"for\" loops and set some parameters for what types of posts I would like to retrieve.  \n",
    "### Skip Ahead:  \n",
    "[Function 1](#Function-1)  \n",
    "[Function 2](#Function-2)  \n",
    "[Epilogue](#Epilogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "#https://stackoverflow.com/questions/25351968/how-to-display-full-non-truncated-dataframe-information-in-html-when-convertin\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1\n",
    "#### get_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "params = {\n",
    "    'subreddit' : 'Xbox',    #subreddit in params will be whatever subreddit is entered into function\n",
    "    'size' : 100          #will pull 100 posts at a time for given subreddit\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referred to lesson 5.04 notes and Pushshift tutorial notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(sub, parameters):  #takes 2 arguments - subreddit and paramater list\n",
    "    #set params\n",
    "    #pushshift url:\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    #appending subreddit url:\n",
    "    r=requests.get(url, parameters)\n",
    "    #break out if status code is not okay\n",
    "    if r.status_code != 200:\n",
    "        return f'Error! Code {r.status_code}.'\n",
    "    #convert data\n",
    "    data = r.json()\n",
    "    posts=data['data']\n",
    "    df = pd.DataFrame(posts)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "df = get_posts('Xbox', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[['subreddit', 'id', 'author', 'num_comments', 'selftext', 'title', 'upvote_ratio', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99    1602785049\n",
       "Name: created_utc, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created_utc'].tail(1) #this is the time stamp from the last post I pulled down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31614804/how-to-delete-a-column-in-pandas-dataframe-based-on-a-condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! This function will pull down 100 posts for a given subreddit and parameters list. Now I need to loop this function append each loop to a single df until I get 10_000 posts. I also want to filter out any posts that will not be useful to my model. In this case, I want \"is_self\" i.e. text posts as opposed to oustide links or photos. So each run-through I'd like to drop any posts in my df where ['is_self'] = False. I'd also like to filter out any [removed] posts which are posts that were deleted after posting. Finally, I need to build in a timestamp to get posts older than the last batch that I pulled before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2\n",
    "#### all_the_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/how-to-create-an-empty-dataframe-and-append-rows-columns-to-it-in-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_the_posts(sub_list): #will pull at least 10k posts from subreddits in list\n",
    "    for sub in sub_list:\n",
    "        params = {'subreddit' : sub,       #subreddit in params will be whatever subreddit is entered into function\n",
    "                    'size' : 100}          #will pull 100 posts at a time for given subreddit\n",
    "        df_m = pd.DataFrame()              #create an empty master data frame\n",
    "        count = 0                          #create a counter to count how many loops I make\n",
    "        while len(df_m) < 10_000:          #loop will run until minimum post goal is met\n",
    "            df_new = get_posts(sub, params)                    #call in the get_posts function, save to df\n",
    "            df_new = df_new[df_new['is_self']==True]           #only keep text posts\n",
    "            df_new = df_new[df_new['selftext']!='[removed]']   #delete any \"removed\" posts\n",
    "            df_m = pd.concat([df_m, df_new])                   #add new df to master df\n",
    "            #print(len(df_m))                                   #print size of master df\n",
    "            count+=1                                           #advance counter by 1\n",
    "            print(f'Loop {count} for {sub} complete; {len(df_m)} posts collected') #print status message\n",
    "            params = {'subreddit' : sub,\n",
    "                     'size':100,\n",
    "                     'before': df_m['created_utc'].min()}      #change params to pull posts older than oldest...\n",
    "            time.sleep(random.randint(4,11))                   #...post in master df + sleep between d/l's\n",
    "        #loop done, I only need to save relevant columns (selected below)\n",
    "        df_m = df_m[['subreddit', 'id', 'author', 'num_comments', 'selftext', 'title', 'upvote_ratio', 'url']]\n",
    "        df_m.to_csv(f'./sub_data/{sub}_10k.csv', index=False)  #after acquiring 10k posts, save to csv\n",
    "    return 'Data collection complete'        #return completion message when both csv's are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_the_posts(['Xbox', 'Playstation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epilogue  \n",
    "That did it! I was able to collect 10,005 rows of Xbox data in 171 loops and 10,0021 rows of Playstation data in 237 loops - automatically filtering out non-text and [removed] posts in the process and only keeping the relevant text and identifying features. I now have my two csv files, Xbox_10k and Playstation_10k which I can combine and analyze to create a model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
